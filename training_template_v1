# -*- coding: utf-8 -*-
"""
Created on Sun Apr  3 13:06:10 2022

@author: Ark
"""

import rlcard
from rlcard.agents import DQNAgent, NFSPAgent, RandomAgent
from odds_agent_21 import OddsAgentV21
from rlcard.utils import tournament,reorganize,Logger,plot_curve

epochs = 5000


env = rlcard.make('limit-holdem', config={
    'seed': 123
    ,'allow_step_back': True
    ,'game_num_players': 2
    })

print("Number of actions:", env.num_actions)
print("Number of players:", env.num_players)
print("Shape of state:", env.state_shape)
print("Shape of action:", env.action_shape)

'''
#DQN 
agent = DQNAgent(num_actions=env.num_actions,
                 state_shape=env.state_shape[0],
                 mlp_layers=[64,64])
'''
#NFSP
agent = NFSPAgent(num_actions=env.num_actions,
                 state_shape=env.state_shape[0],
                 hidden_layers_sizes = [64,64],
                 q_mlp_layers=[64,64],
                 evaluate_with = 'best_response')

#Benchmark (fixed) agent
agent_bench = OddsAgentV21(num_actions=env.num_actions)
#agent_bench = RandomAgent(num_actions=env.num_actions)

agents = []
for _ in range(0, env.num_players-1):
    agents.append(agent)

for _ in range(0, 1):
    agents.append(agent_bench)    
    
env.set_agents(agents)

with Logger("experiments/leduc_holdem_dqn_result/") as logger:
    for episode in range(epochs):

        # Generate data from the environment
        trajectories, payoffs = env.run(is_training=True)

        # Reorganaize the data to be state, action, reward, next_state, done
        trajectories = reorganize(trajectories, payoffs)

        # Feed transitions into agent memory, and train the agent
        for ts in trajectories[0]:
            agent.feed(ts)

        # Evaluate the performance.
        if episode % 50 == 0:
            logger.log_performance(
                env.timestep,
                tournament(
                    env,
                    10000,
                )[0]
            )

    # Get the paths
    csv_path, fig_path = logger.csv_path, logger.fig_path
    
plot_curve(csv_path, fig_path, "agent_training")


# testing in tournament
games = 5
rounds = 1000

print('\n games:',games,' rounds:',rounds)
print(agents)
for i in range(0,games):
    print('game:',i)
    print(tournament(env, rounds))
    
    
# testing in tournament vs random
agent_copy = agent
agents = [agent,RandomAgent(num_actions=env.num_actions)]
env.set_agents(agents)    
print('\n agent_training vs random agent') 
print('games:',games,' rounds:',rounds)
print(agents)
for i in range(0,games):
    print('game:',i)
    print(tournament(env, rounds))    





